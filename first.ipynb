{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97436,"databundleVersionId":11597044,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependencies & Configurations","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip --quiet\n\n!pip install google-generativeai --quiet\n!pip install langgraph pandas --quiet\n!pip install sentencepiece protobuf datasets --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T08:36:58.806801Z","iopub.execute_input":"2025-03-29T08:36:58.807015Z","iopub.status.idle":"2025-03-29T08:37:15.886504Z","shell.execute_reply.started":"2025-03-29T08:36:58.806996Z","shell.execute_reply":"2025-03-29T08:37:15.885583Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport google.generativeai as genai\nfrom google.generativeai import types\nfrom google.api_core import exceptions as google_api_exceptions # For API error handling\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, List, Dict, Optional\nimport ast\nimport re\nimport logging\nimport warnings\nimport os\nimport time\nfrom IPython.display import display\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\nos.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\nprint(\"GEMINI_API_KEY found in Kaggle Secrets.\")\n\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s')\nwarnings.filterwarnings(\"ignore\", category=UserWarning) # General user warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T08:51:50.674052Z","iopub.execute_input":"2025-03-29T08:51:50.674386Z","iopub.status.idle":"2025-03-29T08:51:50.834391Z","shell.execute_reply.started":"2025-03-29T08:51:50.674363Z","shell.execute_reply":"2025-03-29T08:51:50.833754Z"}},"outputs":[{"name":"stdout","text":"GEMINI_API_KEY found in Kaggle Secrets.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 3: Configuration\nAPI_MODEL_NAME = \"gemma-3-27b-it\"\n\nINPUT_DIR = \"/kaggle/input/fpt-ai-residency-batch-6-entry-test\"\nTEST_DATA_PATH = os.path.join(INPUT_DIR, \"b6_test_data.csv\")\nSUBMISSION_PATH = \"submission.csv\"\n\nAPI_TEMPERATURE = 0.2\nAPI_MAX_OUTPUT_TOKENS = 1500\n\nPROCESS_LIMIT = None\n\nMAX_RETRIES = 3\nRETRY_DELAY_SECONDS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:31.801052Z","iopub.execute_input":"2025-03-29T09:03:31.801360Z","iopub.status.idle":"2025-03-29T09:03:31.805403Z","shell.execute_reply.started":"2025-03-29T09:03:31.801336Z","shell.execute_reply":"2025-03-29T09:03:31.804636Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"genai.configure(api_key=GEMINI_API_KEY)\nclient = genai.GenerativeModel(model_name=API_MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:34.354809Z","iopub.execute_input":"2025-03-29T09:03:34.355107Z","iopub.status.idle":"2025-03-29T09:03:34.359346Z","shell.execute_reply.started":"2025-03-29T09:03:34.355083Z","shell.execute_reply":"2025-03-29T09:03:34.358432Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def run_llm_api(prompt: str, model_name: str) -> str:\n    \"\"\"\n    Sends a prompt to the configured Google GenAI model and returns the text response.\n    Includes basic retry logic.\n    \"\"\"\n    retries = 0\n    while retries < MAX_RETRIES:\n        try:\n            model = genai.GenerativeModel(model_name)\n\n            generation_config = types.GenerationConfig(\n                candidate_count=1,\n                max_output_tokens=API_MAX_OUTPUT_TOKENS,\n                temperature=API_TEMPERATURE,\n                top_p=0.95,\n                top_k=64,\n            )\n\n            safety_settings = {\n                types.HarmCategory.HARM_CATEGORY_HARASSMENT: 'BLOCK_MEDIUM_AND_ABOVE',\n                types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'BLOCK_MEDIUM_AND_ABOVE',\n                types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'BLOCK_MEDIUM_AND_ABOVE',\n                types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'BLOCK_MEDIUM_AND_ABOVE',\n            }\n\n            response = model.generate_content(\n                contents=[prompt], # Gemini API expects list of prompts/turns\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n            )\n\n            if not response.candidates:\n                block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else 'Unknown'\n                error_msg = f\"API call blocked or returned no candidates. Reason: {block_reason}\"\n                logging.warning(error_msg)\n                if block_reason == types.BlockReason.SAFETY and retries < MAX_RETRIES -1:\n                     logging.info(f\"Retrying ({retries + 1}/{MAX_RETRIES})...\")\n                     retries += 1\n                     time.sleep(RETRY_DELAY_SECONDS)\n                     continue\n                else:\n                     return f\"[[ERROR: {error_msg}]]\"\n\n\n            generated_text = response.candidates[0].content.parts[0].text\n            logging.debug(f\"API Response (raw): {generated_text[:200]}...\")\n            return generated_text.strip()\n\n        except google_api_exceptions.ResourceExhausted as e:\n            logging.warning(f\"API Quota Error: {e}. Retrying ({retries + 1}/{MAX_RETRIES}) after delay...\")\n            retries += 1\n            time.sleep(RETRY_DELAY_SECONDS * (retries + 1))\n        except Exception as e:\n            logging.error(f\"LLM API call failed unexpectedly: {e}\", exc_info=True)\n            if retries < MAX_RETRIES - 1:\n                 logging.warning(f\"Retrying ({retries + 1}/{MAX_RETRIES}) after general error...\")\n                 retries += 1\n                 time.sleep(RETRY_DELAY_SECONDS)\n            else:\n                logging.error(\"Max retries reached for API call.\")\n                return f\"[[ERROR: API call failed after retries: {e}]]\"\n\n    return f\"[[ERROR: API call failed after {MAX_RETRIES} retries.]]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:34.636686Z","iopub.execute_input":"2025-03-29T09:03:34.636897Z","iopub.status.idle":"2025-03-29T09:03:34.644123Z","shell.execute_reply.started":"2025-03-29T09:03:34.636879Z","shell.execute_reply":"2025-03-29T09:03:34.643378Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def parse_choices(choices_str: str) -> (Dict[str, str], Optional[str]):\n    \"\"\"Safely parses the choices string into a dictionary {A: choice1, B: choice2,...}.\"\"\"\n    try:\n        choices_list = ast.literal_eval(choices_str)\n        if not isinstance(choices_list, list):\n            return {}, \"Parsed choices is not a list.\"\n        max_choices = min(len(choices_list), 26)\n        if max_choices == 0: return {}, \"Parsed choices list is empty.\"\n        parsed = {chr(ord('A') + i): str(choice) for i, choice in enumerate(choices_list[:max_choices])}\n        return parsed, None\n    except (ValueError, SyntaxError, TypeError) as e:\n        logging.warning(f\"ast.literal_eval failed: {e}. Trying regex fallback.\")\n        try:\n            matches = re.findall(r\"['\\\"](.*?)['\\\"]\", choices_str)\n            if matches and len(matches) <= 26:\n                parsed = {chr(ord('A') + i): str(choice) for i, choice in enumerate(matches)}\n                return parsed, f\"Warning: Used regex fallback due to error: {e}\"\n            else:\n                 return {}, f\"Failed to parse choices: {e}. Regex fallback failed.\"\n        except Exception as re_e:\n            return {}, f\"Primary parsing failed: {e}; Regex fallback failed: {re_e}\"\n\ndef format_choices_for_prompt(parsed_choices: Dict[str, str]) -> str:\n    \"\"\"Formats choices for LLM prompts.\"\"\"\n    return \"\\n\".join([f\"({k}) {v}\" for k, v in parsed_choices.items()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:34.768595Z","iopub.execute_input":"2025-03-29T09:03:34.768815Z","iopub.status.idle":"2025-03-29T09:03:34.775205Z","shell.execute_reply.started":"2025-03-29T09:03:34.768797Z","shell.execute_reply":"2025-03-29T09:03:34.774303Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"class AgentState(TypedDict):\n    task_id: str                 # Unique ID for the question\n    question: str                # The question text\n    choices_str: str             # The raw choices string (e.g., \"['10', '9', '8']\")\n    parsed_choices: Dict[str, str] # Parsed choices like {'A': '10', 'B': '9', ...}\n    analyses: Dict[str, str]     # Dictionary to store outputs from analysis nodes\n    critique: Optional[str]      # Stores the synthesized critique of options\n    final_answer: Optional[str]  # The final predicted letter (A, B, C, or D)\n    error: Optional[str]         # Stores error messages if any step fails\n    log: List[str]               # A running log of actions for debugging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:34.901498Z","iopub.execute_input":"2025-03-29T09:03:34.901726Z","iopub.status.idle":"2025-03-29T09:03:34.905704Z","shell.execute_reply.started":"2025-03-29T09:03:34.901708Z","shell.execute_reply":"2025-03-29T09:03:34.904830Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def parse_input_node(state: AgentState) -> AgentState:\n    log = state.get('log', [])\n    task_id = state['task_id']\n    log.append(f\"--- Starting Task {task_id} ---\")\n    log.append(f\"Parsing input choices: {state['choices_str']}\")\n    parsed_choices, error_msg = parse_choices(state['choices_str'])\n    if not parsed_choices:\n        state['error'] = f\"Critical: Could not parse choices string. {error_msg}\"\n        state['final_answer'] = \"A\"\n        log.append(f\"ERROR: Critical choice parsing failed for task {task_id}. Defaulting final answer to A.\")\n    else:\n        state['parsed_choices'] = parsed_choices\n        if error_msg: log.append(error_msg)\n        log.append(f\"Parsed Choices: {parsed_choices}\")\n    state['log'] = log\n    state['analyses'] = {}\n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:35.026837Z","iopub.execute_input":"2025-03-29T09:03:35.027070Z","iopub.status.idle":"2025-03-29T09:03:35.031828Z","shell.execute_reply.started":"2025-03-29T09:03:35.027050Z","shell.execute_reply":"2025-03-29T09:03:35.030996Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"# Agent Definitions & Graph Building","metadata":{}},{"cell_type":"code","source":"# Agent Node Definitions\n\ndef analyze_question_node(state: AgentState) -> AgentState:\n    if state.get('error'): return state\n    log = state['log']\n    log.append(\"Node: Analyzing Question...\")\n    if not state.get('parsed_choices'):\n        state['error'] = \"Cannot analyze question without parsed choices.\"; log.append(state['error']); return state\n\n    formatted_choices = format_choices_for_prompt(state['parsed_choices'])\n    prompt = f\"\"\"You are an expert programming problem analyst.\nAnalyze the following multiple-choice question about programming/software engineering. Do not attempt to answer it yet.\nFocus on:\n1. Rephrasing the core problem or concept being tested in one sentence.\n2. Identifying key code snippets, terms, or programming principles involved.\n3. Briefly assessing the type of question (e.g., syntax error, execution prediction, conceptual understanding, debugging logic).\n\nQuestion:\n{state['question']}\n\nChoices:\n{formatted_choices}\n\nYour analysis:\"\"\"\n    analysis = run_llm_api(prompt, API_MODEL_NAME)\n\n    if \"[[ERROR:\" in analysis:\n        log.append(f\"ERROR detected during question analysis: {analysis}\")\n        state['error'] = f\"API failed during question analysis: {analysis}\"\n    else:\n        state['analyses']['question_analysis'] = analysis\n        log.append(f\"Question Analysis Result:\\n{analysis[:300]}...\")\n    state['log'] = log\n    return state\n\ndef evaluate_options_node(state: AgentState) -> AgentState:\n    if state.get('error'): return state\n    log = state['log']\n    log.append(\"Node: Evaluating Options...\")\n    if not state.get('parsed_choices'):\n        state['error'] = \"Cannot evaluate options without parsed choices.\"; log.append(state['error']); return state\n\n    formatted_choices = format_choices_for_prompt(state['parsed_choices'])\n    question_analysis = state['analyses'].get('question_analysis', 'Analysis not available.')\n    if \"[[ERROR:\" in question_analysis: question_analysis = \"Previous analysis failed.\"\n\n    prompt = f\"\"\"You are a meticulous and unbiased code and logic evaluator.\nBased on the following question and its initial analysis, evaluate EACH multiple-choice option step-by-step. For every single option (A, B, C, D...):\n1. State clearly whether the option seems correct or incorrect.\n2. Provide a detailed, step-by-step explanation for your conclusion, referencing the question, code (if any), and relevant programming principles.\n3. If code execution is relevant, trace the logic. If it's conceptual, explain the concept.\n**CRITICAL**: Evaluate each option based purely on its merits relative to the question. Do NOT let the position (A vs B vs C vs D) influence your judgment or the thoroughness of your evaluation. Be equally critical of all options.\n\nQuestion:\n{state['question']}\n\nInitial Analysis:\n{question_analysis}\n\nChoices to Evaluate:\n{formatted_choices}\n\nYour detailed evaluation for EACH option:\"\"\"\n    evaluation = run_llm_api(prompt, API_MODEL_NAME)\n\n    if \"[[ERROR:\" in evaluation:\n        log.append(f\"ERROR detected during options evaluation: {evaluation}\")\n        state['error'] = f\"API failed during options evaluation: {evaluation}\"\n    else:\n        state['analyses']['options_evaluation'] = evaluation\n        log.append(f\"Options Evaluation Result:\\n{evaluation[:300]}...\")\n    state['log'] = log\n    return state\n\ndef synthesize_critique_node(state: AgentState) -> AgentState:\n    if state.get('error'): return state\n    log = state['log']\n    log.append(\"Node: Synthesizing & Critiquing...\")\n    if not state.get('parsed_choices'):\n        state['error'] = \"Cannot synthesize without parsed choices.\"; log.append(state['error']); return state\n\n    formatted_choices = format_choices_for_prompt(state['parsed_choices'])\n    question_analysis = state['analyses'].get('question_analysis', 'Analysis not available.')\n    options_evaluation = state['analyses'].get('options_evaluation', 'Evaluation not available.')\n    if \"[[ERROR:\" in question_analysis: question_analysis = \"Previous analysis failed.\"\n    if \"[[ERROR:\" in options_evaluation: options_evaluation = \"Previous evaluation failed.\"\n\n    prompt = f\"\"\"You are a lead analyst responsible for making a final recommendation based on prior analysis.\nReview the initial question analysis and the detailed option-by-option evaluation provided below. Your task is to synthesize these findings into a final critique:\n1. Clearly state which option (A, B, C, D...) appears to be the most correct based *solely* on the provided evaluations.\n2. Briefly summarize the primary reason(s) why this option is favored, according to the evaluation.\n3. Briefly mention the key reason(s) why the other options were deemed incorrect, according to the evaluation.\n4. Assess the overall confidence level suggested by the evaluation (e.g., High, Medium, Low).\n\nQuestion:\n{state['question']}\n\nChoices:\n{formatted_choices}\n\nInitial Analysis:\n{question_analysis}\n\nOption-by-Option Evaluation:\n{options_evaluation}\n\nYour Synthesis and Final Critique:\"\"\"\n    critique = run_llm_api(prompt, API_MODEL_NAME)\n\n    if \"[[ERROR:\" in critique:\n        log.append(f\"ERROR detected during critique synthesis: {critique}\")\n        state['error'] = f\"API failed during critique synthesis: {critique}\"\n    else:\n        state['critique'] = critique\n        log.append(f\"Synthesis/Critique Result:\\n{critique[:300]}...\")\n    state['log'] = log\n    return state\n\ndef final_decision_node(state: AgentState) -> AgentState:\n    if state.get('error'):\n        if not state.get('final_answer'): state['final_answer'] = \"A\"; state['log'].append(\"Setting final answer to default 'A' due to prior error.\")\n        return state\n\n    log = state['log']\n    log.append(\"Node: Making Final Decision...\")\n    if not state.get('parsed_choices'):\n        state['error'] = \"Cannot make decision without parsed choices.\"; log.append(state['error']); state['final_answer'] = \"A\"; return state\n\n    critique = state.get('critique', 'Critique not available.')\n    if \"[[ERROR:\" in critique or critique == 'Critique not available.':\n        log.append(f\"ERROR: Critique unavailable or contains error ({critique[:50]}...). Defaulting final answer.\")\n        state['error'] = \"Critique unavailable/error.\"; state['final_answer'] = \"A\"; return state\n\n    valid_choices = list(state['parsed_choices'].keys())\n\n    prompt = f\"\"\"You are an expert analyst tasked with selecting the single best answer from a multiple-choice list based *only* on the provided critique, even if the critique is uncertain or finds flaws in all options.\n\nYour task:\n1. Review the 'Synthesis and Critique' below.\n2. Identify which choice ({', '.join(valid_choices)}) is determined to be the most plausible or least incorrect according to the critique's reasoning.\n3. You *MUST* output the single capital letter corresponding to that choice.\n4. Output *ONLY* the letter. Do NOT include any other words, explanation, punctuation, or refusal. Just the single letter.\n\nSynthesis and Critique:\n{critique}\n\nThe single best choice letter is:\"\"\"\n\n    decision_raw = run_llm_api(prompt, API_MODEL_NAME)\n\n    if \"[[ERROR:\" in decision_raw:\n        log.append(f\"ERROR detected during final decision API call: {decision_raw}\")\n        state['error'] = f\"API failed during final decision: {decision_raw}\"\n        state['final_answer'] = \"A\" # Fallback\n        state['log'] = log\n        return state\n\n    final_answer = None\n    log_msg = f\"Attempting to parse decision from API output: '{decision_raw}'\"\n    match_strict = re.search(r'(?::\\s*|>\\s*|^)([A-Z])\\b', decision_raw.strip())\n    match_paren = re.search(r'\\(([A-Z])\\)', decision_raw)\n    match_standalone = re.search(r'\\b([A-Z])\\b', decision_raw)\n    match_punct = re.search(r'^([A-Z])[.)]', decision_raw.strip())\n    cleaned_decision = decision_raw.strip().upper()\n\n    if match_strict and match_strict.group(1) in valid_choices: final_answer = match_strict.group(1); log_msg += f\" -> Parsed '{final_answer}' (strict).\"\n    elif match_paren and match_paren.group(1) in valid_choices: final_answer = match_paren.group(1); log_msg += f\" -> Parsed '{final_answer}' (paren).\"\n    elif match_standalone and match_standalone.group(1) in valid_choices: final_answer = match_standalone.group(1); log_msg += f\" -> Parsed '{final_answer}' (standalone).\"\n    elif match_punct and match_punct.group(1) in valid_choices: final_answer = match_punct.group(1); log_msg += f\" -> Parsed '{final_answer}' (punct).\"\n    elif len(cleaned_decision) == 1 and cleaned_decision in valid_choices: final_answer = cleaned_decision; log_msg += f\" -> Parsed '{final_answer}' (direct).\"\n    else: final_answer = \"A\"; log_msg += f\" -> ERROR: Could not parse single letter. Defaulting 'A'.\"; state['error'] = f\"Failed to parse final decision: '{decision_raw}'\"\n\n    state['final_answer'] = final_answer\n    log.append(log_msg)\n    log.append(f\"--- Task {state['task_id']} Finished ---\")\n    state['log'] = log\n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:35.266510Z","iopub.execute_input":"2025-03-29T09:03:35.266782Z","iopub.status.idle":"2025-03-29T09:03:35.280995Z","shell.execute_reply.started":"2025-03-29T09:03:35.266762Z","shell.execute_reply":"2025-03-29T09:03:35.280110Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Graph Building Function\n\ndef build_graph():\n    \"\"\"Builds the LangGraph StateGraph using API calls.\"\"\"\n    graph_builder = StateGraph(AgentState)\n    graph_builder.add_node(\"parse_input\", parse_input_node)\n    graph_builder.add_node(\"analyze_question\", analyze_question_node)\n    graph_builder.add_node(\"evaluate_options\", evaluate_options_node)\n    graph_builder.add_node(\"synthesize_critique\", synthesize_critique_node)\n    graph_builder.add_node(\"final_decision\", final_decision_node)\n\n    graph_builder.set_entry_point(\"parse_input\")\n    graph_builder.add_edge(\"parse_input\", \"analyze_question\")\n    graph_builder.add_edge(\"analyze_question\", \"evaluate_options\")\n    graph_builder.add_edge(\"evaluate_options\", \"synthesize_critique\")\n    graph_builder.add_edge(\"synthesize_critique\", \"final_decision\")\n    graph_builder.add_edge(\"final_decision\", END)\n\n    app = graph_builder.compile()\n    return app","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:35.370709Z","iopub.execute_input":"2025-03-29T09:03:35.370937Z","iopub.status.idle":"2025-03-29T09:03:35.375533Z","shell.execute_reply.started":"2025-03-29T09:03:35.370918Z","shell.execute_reply":"2025-03-29T09:03:35.374818Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Build Graph\n\napp = build_graph()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:35.476746Z","iopub.execute_input":"2025-03-29T09:03:35.476962Z","iopub.status.idle":"2025-03-29T09:03:35.491070Z","shell.execute_reply.started":"2025-03-29T09:03:35.476944Z","shell.execute_reply":"2025-03-29T09:03:35.490354Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"test_df = pd.read_csv(TEST_DATA_PATH)\nrequired_cols = ['task_id', 'question', 'choices']\nif not all(col in test_df.columns for col in required_cols):\n    raise ValueError(f\"Test CSV must contain columns: {required_cols}\")\ndisplay(test_df)\n\nif PROCESS_LIMIT is not None and PROCESS_LIMIT > 0:\n    test_df = test_df.head(PROCESS_LIMIT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:35.593402Z","iopub.execute_input":"2025-03-29T09:03:35.593617Z","iopub.status.idle":"2025-03-29T09:03:35.619680Z","shell.execute_reply.started":"2025-03-29T09:03:35.593598Z","shell.execute_reply":"2025-03-29T09:03:35.618974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"     task_id                                           question  \\\n0     k10171  Question: What will be output of the following...   \n1     k10182  Question: Consider line 3. Identify the compil...   \n2     k10184  Question: Assume the conflicts part (a) of thi...   \n3     k10206  Question: What will be output if you will exec...   \n4     k10215  Question: Select the output for code :\\nstatic...   \n...      ...                                                ...   \n1248  k00687  Question: Which React lifecycle method is call...   \n1249  k00689  Question: What do you need to change about thi...   \n1250  k00691  Question: What happens when the following rend...   \n1251  k00699  Question: How many types of components are the...   \n1252  k00700  Question: How do you access the updated state ...   \n\n                                                choices  \n0                             ['10', '9', '8', 'Error']  \n1     ['No compilation error', 'Only a lexical error...  \n2     ['Equal precedence and left associativity; exp...  \n3     ['2.00000', '4.00000', '6.00000', 'Compilation...  \n4     ['amish', 'ANKIT', 'harsh', 'Compile time error']  \n...                                                 ...  \n1248  ['componentWillMount', 'componentDidMount', 'c...  \n1249  ['Remove this', 'Capitalize clock', 'Remove th...  \n1250  ['Error. Cannot use direct JavaScript code in ...  \n1251                               ['1', '2', '3', '4']  \n1252  [\"It's automatically available in the next lin...  \n\n[1253 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task_id</th>\n      <th>question</th>\n      <th>choices</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>k10171</td>\n      <td>Question: What will be output of the following...</td>\n      <td>['10', '9', '8', 'Error']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>k10182</td>\n      <td>Question: Consider line 3. Identify the compil...</td>\n      <td>['No compilation error', 'Only a lexical error...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>k10184</td>\n      <td>Question: Assume the conflicts part (a) of thi...</td>\n      <td>['Equal precedence and left associativity; exp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>k10206</td>\n      <td>Question: What will be output if you will exec...</td>\n      <td>['2.00000', '4.00000', '6.00000', 'Compilation...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>k10215</td>\n      <td>Question: Select the output for code :\\nstatic...</td>\n      <td>['amish', 'ANKIT', 'harsh', 'Compile time error']</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1248</th>\n      <td>k00687</td>\n      <td>Question: Which React lifecycle method is call...</td>\n      <td>['componentWillMount', 'componentDidMount', 'c...</td>\n    </tr>\n    <tr>\n      <th>1249</th>\n      <td>k00689</td>\n      <td>Question: What do you need to change about thi...</td>\n      <td>['Remove this', 'Capitalize clock', 'Remove th...</td>\n    </tr>\n    <tr>\n      <th>1250</th>\n      <td>k00691</td>\n      <td>Question: What happens when the following rend...</td>\n      <td>['Error. Cannot use direct JavaScript code in ...</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>k00699</td>\n      <td>Question: How many types of components are the...</td>\n      <td>['1', '2', '3', '4']</td>\n    </tr>\n    <tr>\n      <th>1252</th>\n      <td>k00700</td>\n      <td>Question: How do you access the updated state ...</td>\n      <td>[\"It's automatically available in the next lin...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1253 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"# Process Questions\n\nresults = []\ntotal_rows = len(test_df)\n\nfor index, row in test_df.iterrows():\n    task_id = row['task_id']\n    logging.info(f\"--- Processing Task ID: {task_id} ({index + 1}/{total_rows}) ---\")\n    initial_state = {\n        \"task_id\": task_id,\n        \"question\": str(row['question']),\n        \"choices_str\": str(row['choices']),\n        \"parsed_choices\": {}, \"analyses\": {}, \"critique\": None,\n        \"final_answer\": None, \"error\": None, \"log\": []\n    }\n\n    final_state = None\n    try:\n        final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n\n        current_answer = final_state.get('final_answer', 'A')\n        current_error = final_state.get('error')\n        current_log = final_state.get('log', [])\n\n        results.append({\n            \"task_id\": final_state['task_id'],\n            \"answer\": current_answer\n        })\n\n        if current_error:\n            logging.warning(f\"Task {task_id} completed with error state: {current_error}\")\n            print(f\"--- LOG FOR ERRORED TASK {task_id} ---\\n\" + \"\\n\".join(current_log) + \"\\n--- END LOG ---\")\n\n    except Exception as e:\n        logging.error(f\"CRITICAL error invoking graph for task {task_id}: {e}\", exc_info=True)\n        results.append({\"task_id\": task_id, \"answer\": \"A\"}) # Default 'A'\n        log_to_print = final_state['log'] if final_state and final_state.get('log') else initial_state.get('log', [])\n        if log_to_print: print(f\"--- LOG FOR CRASHED TASK {task_id} ---\\n\" + \"\\n\".join(log_to_print) + \"\\n--- END LOG ---\")\n\nlogging.info(f\"Finished processing {len(results)} questions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:03:35.688531Z","iopub.execute_input":"2025-03-29T09:03:35.688775Z"}},"outputs":[{"name":"stdout","text":"--- LOG FOR ERRORED TASK k10323 ---\n--- Starting Task k10323 ---\nParsing input choices: ['the child process kills the parent process', 'the parent process kills the child process', 'handler function executes as the signal arrives to the parent process', 'none of the mentioned']\nParsed Choices: {'A': 'the child process kills the parent process', 'B': 'the parent process kills the child process', 'C': 'handler function executes as the signal arrives to the parent process', 'D': 'none of the mentioned'}\nNode: Analyzing Question...\nQuestion Analysis Result:\n## Question Analysis\n\n1. **Core Problem:** The question tests understanding of process creation (fork), signal handling (specifically SIGKILL), and the consequences of a child process terminating its parent.\n\n2. **Key Elements:**\n   * `fork()`: Creates a child process.\n   * `kill(getppid(), SIGKILL)...\nNode: Evaluating Options...\nERROR detected during options evaluation: [[ERROR: API call failed after retries: list index (0) out of range]]\nSetting final answer to default 'A' due to prior error.\n--- END LOG ---\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}